<h1> MLCrawler 6262 Project Repository </h1>

**Analysis**
 - blacklist_check.py : shows blacklist deferral that we also crawled in our dataset
 - extract_data.py : stats generation from raw crawled document

**Crawler:**
- provision_main.sh : provision system necessary to run everything
- blacklist
    - blacklist_parser.py : parse and generate single blacklist csv from collect source data with different formatting
    - blacklist_solr.py : post blacklist data into SOLR
- crawler-scrapy
  - alexatop
    - alexatop
      - spiders
        - alexa_spider.py : Alexa Spider to crawl alexa list
      - items.py : Crawled Document class
      - middlewares.py
      - pipelines.py
      - settings.py : conifguration of log and data directory
      - util.py : helper function on parsing crawled urls
    - run_alexa.py : divide 1M alexa list into 10 different files and initiate crawler in seperate process
    - scrapy.cfg : scrapy project config
    - data : data repo for crawled items
    - logs : scrapy run log

**Resources:**
  - pbl_crawl.sh: cronbased daily blacklist fetch script
  - run_alexa_crawl.sh : cronbased daily alexa startup script
  - crawler_stat.py : generate crawl trend graph from stats generated by extract_data.py at alanysis folder

<h2> How to use this code </h2>

**Prerequisite:**: 
Install provisioning script
```$ ./crawler/provision_main.sh```

**tuning the system** (use local DNS cache & increase max open file limit):
```## configuring dnsmasq for dns caching
$ sudo vim /etc/NetworkManager/dnsmasq.d/cache
#write following on the cache file
cache-size=1000000
#restart network manager to affect cache
$ sudo restart network-manager
#check if its working
$ sudo zgrep dnsmasq /var/log/syslog* | grep dnsmasq

#increasing max file open limit (ulimit -n) for crawler user
$ sudo vim /etc/security/limits.conf
*               soft    nofile          65535
*               hard    nofile          65535
#change this to effect ulimit without restart (then logout and login)
$ sudo vim /etc/pam.d/common-session
session     required    pam_limits.so
```

**Running the crawler**
```
#use tmux so crawl don't stop once you logoff
$ tmux #or "tmux a -t [id]" if you have a session already. To detach ctrl+b->d at the end
# activate python3 in virtual environment
$ source ~/venv/bin/activate 
$ cd ~/mlcrawler6262/crawler/crawler-scrapy/alexatop/
$ sh run_alexa_crawl.sh
#you will find data at ./data
```
**Post Analysis & Data Loading**
```
$ source ~/venv/bin/activate  #activate python 3.X
# start SOLR on port 8983 (default)
$ solr start
# create 2 schema for crawl data and 
bin/solr create_core -c search

# Process Blacklist files to dump into single file (data.json) and upload to solr
$ cd crawler/blacklist/
$ sh blacklist_solr.sh

# post analysis data generation
$ cd analysis/
# this will generate json files, ready to post into SOLR
$ python extract_data.py
# post data into solr
$ SOLR_HOME/bin/post -c search output/YOUR_FILE_NAME.json
```
**Visualization System**
TODO
