<h1> MLCrawler 6262 Project Repository </h1>

**Analysis**
 - blacklist_check.py : shows blacklist deferral that we also crawled in our dataset
 - extract_data.py : stats generation from raw crawled document

**Crawler:**
- provision_main.sh : provision system necessary to run everything
- blacklist
    - blacklist_parser.py : parse and generate single blacklist csv from collect source data with different formatting
    - blacklist_solr.py : post blacklist data into SOLR
- crawler-scrapy
  - alexatop
    - alexatop
      - spiders
        - alexa_spider.py : Alexa Spider to crawl alexa list
      - items.py : Crawled Document class
      - middlewares.py
      - pipelines.py
      - settings.py : conifguration of log and data directory
      - util.py : helper function on parsing crawled urls
    - run_alexa.py : divide 1M alexa list into 10 different files and initiate crawler in seperate process
    - scrapy.cfg : scrapy project config
    - data : data repo for crawled items
    - logs : scrapy run log

**Resources:**
  - pbl_crawl.sh: cronbased daily blacklist fetch script
  - run_alexa_crawl.sh : cronbased daily alexa startup script
  - crawler_stat.py : generate crawl trend graph from stats generated by extract_data.py at alanysis folder

<h2> How to use this code </h2>

**Prerequisite:**: 
Install provisioning script
```$ ./crawler/provision_main.sh```

**tuning the system** (use local DNS cache & increase max open file limit):
```## configuring dnsmasq for dns caching
$ sudo vim /etc/NetworkManager/dnsmasq.d/cache
#write following on the cache file
cache-size=1000000
#restart network manager to affect cache
$ sudo restart network-manager
#check if its working
$ sudo zgrep dnsmasq /var/log/syslog* | grep dnsmasq

#increasing max file open limit (ulimit -n) for crawler user
$ sudo vim /etc/security/limits.conf
*               soft    nofile          65535
*               hard    nofile          65535
#change this to effect ulimit without restart (then logout and login)
$ sudo vim /etc/pam.d/common-session
session     required    pam_limits.so
```


**Running the crawler**
```$ cd 
